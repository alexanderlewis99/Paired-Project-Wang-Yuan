---
title: "R Notebook"
output: html_notebook
author: "Alec Wang and Allen Yuan"
---

## Problem 1. Derivations.
Let $X_1,\ldots,X_n$ be a random sample of observed detection distances from a half-normal distribution with pdf:
$f(x)=\frac{2}{\sqrt{2\pi\sigma}}e^{-x^2/(2\sigma^2)}$ for $x\geq 0$.
The expected value of $X$ is $E(X)=\sigma\sqrt{\frac{2}{\pi}}$ and the variance is $V(X)=\sigma^2(1-2/\pi).$

Note that this density is simply twice the pdf of a $N(0,\sigma^2)$ density and we can obtain a half-normal random variable $X$ by taking the absolute value of $Y\sim N(0,\sigma^2)(i.e. X=|Y|).$

#### Part A
The day 11 MLE examples (Wednesday of week 4) gives you the MLE $\hat\sigma_{MLE}$ for $\sigma$. The square of this MLE, $\hat\sigma^2_{MLE}$ is the MLE for $\sigma^2$. Derive the sampling distribution of $\hat\sigma^2_{MLE}$. (Hint: Use your probability Appendix theorems and you should end up with a Gamma!)

From the MLE examples, we have that $\hat\sigma_{MLE}^2= \frac{\sum X^2_i}{n}$ with $X_i \sim N(0, \sigma^2)$.

Therefore $X_i = Z_i \times\sigma$ with $Z \sim N(0, 1)$.

So $\hat\sigma_{MLE}^2=\frac{\sum (Z_i \times\sigma)^2}{n}$

$=\frac{\sum_{i=1}^n Z_i^2 \times\sigma^2}{n}$

$=n\sigma^2\frac{\sum_{i=1}^n Z_i^2}{n}$

$=\sigma^2\sum_{i=1}^n Z_i^2$.

Now, by Theorem B.10.4, since $Z_1,Z_2,\ldots,Z_k$ are independent random variables from the standard normal distribution, $\sum_{i=1}^{n} Z_i^2$ has a $\chi^2$ distribution with $n$ degrees of freedom.

Therefore $\hat\sigma_{MLE}^2=\sigma^2\times\chi^2_n$.

Since the Chi-Square distribution with $n$ degress of freedom is a Gamma distribution with $r=\frac{n}{2}$ and $\lambda=\frac{1}{2}$, then $\hat\sigma_{MLE}^2=\sigma^2\times Y$ with $Y\sim Gamma(\frac{n}{2},\frac{1}{2})$.

By Proposition B.9.3, $\hat\sigma_{MLE}^2\sim Gamma(\frac{n}{2},\frac{1}{2\sigma^2})$.

#### Part B
Derive the MoM estimator $\hat\sigma^2_{MoM}$ for $\sigma^2$.

The first theoretical moment of $X$ is $E(X)=\sigma\sqrt{\frac{2}{\pi}}$.

The first sample moment of $X$ is $\sum_{i=1}^n x_i=\bar X.$

So, using the methods of moments, we have $\bar X = \sigma\sqrt{\frac{2}{\pi}}$

$\implies \sigma_{MoM} = \bar X/\sqrt{\frac{2}{\pi}} =  \bar X\sqrt{\frac{\pi}{2}}$

$\implies \sigma_{MoM}^2 = \frac{\pi}{2}\bar X^2$.

#### Part C
Determine whether the MLE and MoM estimators of $\sigma^2$  are unbiased. If one, or both, are biased, dervive the bias for that estimator.

Since we proved that $\hat\sigma^2_{MLE}$ is distributed by the Gamma distribution, then  $E[\hat\sigma^2_{MLE}]=r/\lambda=(n/2)/(1/(2\sigma^2))=2\sigma^2n/2=\sigma^2n.$ 

$Bias[\hat\sigma^2_{MLE}] = E[\hat\sigma^2_{MLE}] - \sigma^2$

$=\sigma^2n-\sigma^2$

$=(n-1)\sigma^2$.

Thus, we see that $\hat\sigma^2_{MLE}$ is a biased estimator.

$Bias[\hat\sigma^2_{MoM}] = E[\hat\sigma^2_{MoM}] - \sigma^2$

$=\frac{\pi}{2}\bar X^2 -\sigma^2$.

Since $\bar X = E(X)=\sigma\sqrt{\frac{2}{\pi}}$, then

$Bias[\hat\sigma^2_{MoM}] =\frac{\pi}{2}\times \sigma^2 \times \frac{2}{\pi}-\sigma^2$

$= 0.$

Thus, we see that $\hat\sigma^2_{MoM}$ is an unbiased estimator.

#### Part D
Using your distribution from (a), derive an (exact) 95% confidence interval for $\sigma^2$ using the pivotal method described in chapter 7.
Let $X_1,\ldots,X_n$ be a random sample of observed detection distances from a half-normal distribution. From part A we found the distribution $\hat\sigma_{MLE}^2\sim Gamma(\frac{n}{2},\frac{1}{2\sigma^2})$.

It is Gamma distribution with parameters $\frac{n}{2}$ and $\lambda = \frac{1}{2\sigma^2}$.

By Theorem B.9.4, it proves that the Gamma distribution does not depend of the $\lambda = \frac{1}{2\sigma^2}$ so $\lambda = \frac{1}{2\sigma^2}$ is a pivot. 

For a 95 percent confidence interval $0.95 = P(\frac{q_{0.025}}{\bar x}\leq \lambda \leq \frac{q_{0.975}}{\bar x})$

$= P(\frac{q_{0.025}}{\bar x}\leq \frac{1}{2\sigma^2} \leq \frac{q_{0.975}}{\bar x})$ 

$= P(\frac{2q_{0.025}}{\bar x}\leq \frac{1}{\sigma^2} \leq \frac{2q_{0.975}}{\bar x})$ 

$= P(\frac{\bar x}{2q_{0.025}}\geq \frac{1}{\sigma^2} \geq \frac{\bar x}{2q_{0.975}})$ 

$= P(\frac{\bar x}{2q_{0.975}}\leq \frac{1}{\sigma^2} \leq \frac{\bar x}{2q_{0.025}})$ 

Thus our interval is $(\frac{\bar x}{2q_{0.975}}, \frac{\bar x}{2q_{0.025}})$ where $q_{0.975}$ and $q_{0.025}$ come from the Gamma distribution with $r = n/2$ and $\lambda = \frac{1}{2\sigma^2}$.

## Part 2. Simuations
We want to determine which estimator is “better” by using simulation to help assess estimator properties (like Bias and MSE) along with confidence interval properties (like coverage rate and interval length). Here is the basic simulation set up:

#### Part 1.
Fix a value for $\sigma^2$ and $n$.
```{r}
n <- 100 # sample size
sigma <- 0.05 # value of sigma
```

#### Part 2.
Take a sample of n half normal values using your values from (1):
```{R}
x <- abs(rnorm(n,0,sigma))   # generating half normal random sample
```

#### Part 3
For each sample, compute point estimates
We found that $\hat\sigma_{MLE}^2= \frac{\sum X^2_i}{n}$ with $X_i \sim N(0, \sigma^2)$ and $\sigma_{MoM}^2 = \frac{\pi}{2}\bar X^2$.
```{R}
est_mle <- list()
est_mom <- list()
i <- 1
est_mle[i] <- sum(x^2)/n # MLE formula
est_mom[i] <- mean(x)^2*pi/2 # MoM formula
```

#### Part 4
For each sample, compute the lower and upper bounds for each of three competing CI methods:
We found that for MLE 95% confidence interval is $(\frac{\bar x}{2q_{0.975}}, \frac{\bar x}{2q_{0.025}})$ where $q_{0.975}$ and $q_{0.025}$ come from the Gamma distribution with $r = n/2$ and $\lambda = \frac{1}{2\sigma^2}$.
```{R}
q_0.975 <- qgamma(0.975,shape = n/2, 1/(2*sigma^2))
q_0.025 <- qgamma(0.025,shape = n/2, 1/(2*sigma^2))

# based on exact MLE CI (part d)
x_bar <- mean(x)
mle_lower[i] <- x_bar/(2*q_0.975)
# based on exact MLE CI (part d)
mle_upper[i] <- x_bar/(2*q_0.025)
# CI based on bootstrapping the MLE

set.seed(0)
N <- 10^5
my.boot.MLE <- numeric(N)
my.boot.MoM <- numeric(N)
for(j in 1:N)
{
  s <- sample(x, 100, replace = TRUE) # draw resamples
  my.boot.MLE[j] <- sum(s^2)/n # compute MLE variance, store in my.boot
  my.boot.MoM[j] <- mean(s)^2*pi/2 # compute MoM variance, store in my.boot
}
mle_boot_lower <- list()
mle_boot_upper <- list()
mle_boot_lower[i] <- quantile(my.boot.MLE, c(0.025))
mle_boot_upper[i] <- quantile(my.boot.MLE, c(0.975))
# CI based on bootstrapping the MoM
mom_boot_lower <- list()
mom_boot_upper <- list()
mom_boot_lower[i] <- quantile(my.boot.MoM, c(0.025))
mom_boot_upper[i] <- quantile(my.boot.MoM, c(0.975))
```
Note that you will have to create a bootstrap loop to generate the lower and upper bounds for your bootstrap CI for this one sample.

#### Part 5
Repeat 2-4 many times (`N`). Try at least 10k simulations!

```{R}
n <- 100 # sample size
sigma <- sqrt(0.05) # value of sigma
set.seed(0)
K <- 10^2
est_mle <- c()
est_mom <- c()
mle_lower <- c()
mle_upper <- c()
mle_boot_lower <- c()
mle_boot_upper <- c()
mom_boot_lower <- c()
mom_boot_upper <- c()
for(i in 1:K){
  x <- abs(rnorm(n,0,sigma))   # generating half normal random sample
  est_mle[i] <- sum(x^2)/n # MLE formula
  est_mom[i] <- mean(x)^2*pi/2 # MoM formula
  
  q_0.975 <- qgamma(0.975,shape = n/2, 1/(2*sigma^2))
  q_0.025 <- qgamma(0.025,shape = n/2, 1/(2*sigma^2))
  
  # based on exact MLE CI (part d)
  x_bar <- mean(x)
  mle_lower[i] <- x_bar/(2*q_0.975)
  # based on exact MLE CI (part d)
  mle_upper[i] <- x_bar/(2*q_0.025)
  # CI based on bootstrapping the MLE
  set.seed(0)
  N <- 10^2
  my.boot.MLE <- numeric(N)
  my.boot.MoM <- numeric(N)
  for(j in 1:N)
  {
    s <- sample(x, 100, replace = TRUE) # draw resamples
    my.boot.MLE[j] <- sum(s^2)/n # compute MLE variance, store in my.boot
    my.boot.MoM[j] <- mean(s)^2*pi/2 # compute MoM variance, store in my.boot
  }
  mle_boot_lower[i] <- quantile(my.boot.MLE, c(0.025))
  mle_boot_upper[i] <- quantile(my.boot.MLE, c(0.975))
  
  # CI based on bootstrapping the MoM
  mom_boot_lower[i] <- quantile(my.boot.MoM, c(0.025))
  mom_boot_upper[i] <- quantile(my.boot.MoM, c(0.975))
}
mean(est_mle)
mean(est_mom)
mean(mle_lower)
mean(mle_upper)
mean(mle_boot_lower)
mean(mle_boot_upper)
mean(mom_boot_lower)
mean(mom_boot_upper)
```

#### Part 6
Compare your estimators in (3) to your true value of $\sigma^2$. Simulated bias, percent bias (as a percentage of the true $\sigma^2$) and MSE look like the following (for both MLE and MoM):

```{R}
est_mle.mean <- mean(est_mle)
est_mom.mean <- mean(est_mom)

getEstError <- function(est, sigma){
  bias <- mean(est) - sigma^2  
  perc_bias <- 100*bias/sigma^2  # bias as a percentage of sigma^2
  mse <- mean((est - sigma^2)^2) # mean of squared deviations (est - truth)^2
  res <- list(bias, perc_bias, mse)
  names(res) <- c("bias", "perc_bias", "mse")
  return(res)
}

est_mle.error <- getEstError(est_mle.mean, sigma)
est_mom.error <- getEstError(est_mom.mean, sigma)
```
Note that you can compute the exact bias values for both estimators (part c), so your simulation bias values should be close to your exact values if your simulation size is big.

#### Part 7
Compare interval estimates in (4) by looking at how often they cover the true value of $\sigma^2$ and their mean length:
```{R}
mle_lower.mean <- mean(mle_lower)
mle_upper.mean <- mean(mle_upper)
mle_boot_lower.mean <- mean(mle_boot_lower)
mle_boot_upper.mean <- mean(mle_boot_upper)
mom_boot_lower.mean <- mean(mom_boot_lower)
mom_boot_upper.mean <- mean(mom_boot_upper)

getCoverage <- function(lower, upper, sigma){
  cover <- mean((lower <= sigma^2) & (sigma^2 <= upper))
}
getMeanLength <- function(upper, lower){
  mean.length <- mean(upper - lower)
}

mle.cover <- getCoverage(mle_lower.mean, mle_upper.mean, sigma)
mle.meanLength <- getMeanLength(mle_lower.mean, mle_upper.mean)

mle_boot.cover <- getCoverage(mle_boot_lower.mean, mle_boot_upper.mean, sigma)
mle_boot.meanLength <- getMeanLength(mle_boot_lower.mean, mle_boot_upper.mean)

mom_boot.cover <- getCoverage(mom_boot_lower.mean, mom_boot_upper.mean, sigma)
mom_boot.meanLength <- getMeanLength(mom_boot_lower.mean, mom_boot_upper.mean)
```
We would like to have a coverage rate close to 95% (because that is what we are claiming for our confidence level). If two intervals have rates of about 95%, then we would prefer the interval that is shorter in mean length.

mle.cover does not cover $\sigma^2.$ mle_boot.cover and mom_boot.cover both cover $\sigma^2.$

#### Part 8
Finally, some estimator properties may depend on your initial simulation values ($n$ and $\sigma^2$). You should explore how changing these values affects your conclusions about which point and interval estimators are “best.”
Note: I’ve outlined the minimal criteria for the simulation. Feel free to try other estimators of $\sigma^2$ and/or other ways of computing a 95% confidence interval.

## 3. Estimation for Bergen

Based on your “best” results from parts 1 and 2, provide a point and interval estimate for $\sigma^2$ using the Bergen detection distances. Also use these values to give point and interval estimates for the probability $p_x$ of detecting a mussel that is “x” meters off the transect: $p_x=e^{-x^2/(2\sigma^2)}$.
for “x” distances of 0.5 meters and 1 meter.

Interesting aside: It is these detection probabilities that are then used to estimate how many mussels are in the lake. E.g. if we observe a mussel with a 0.5 probability of being detected, we can reason that there must also be one mussel (at the same distance) that we missed. Hence, this one mussel really represents two mussels in the entire population.
